<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Credit Risk Prediction Tutorial</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
            background-color: #f4f4f9;
            color: #333;
        }
        header {
            text-align: center;
            margin-bottom: 30px;
        }
        header h1 {
            font-size: 1.8em;
            color: #0046ad;
            margin-bottom: 10px;
        }
        header h3 {
            font-size: 1.1em;
            color: #333;
            margin: 5px 0;
        }
        section {
            background: #ffffff;
            padding: 20px;
            margin-bottom: 20px;
            border-radius: 5px;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
        }
        section h2 {
            font-size: 1.5em;
            color: #0046ad;
            margin-bottom: 15px;
            border-bottom: 2px solid #0046ad;
            padding-bottom: 5px;
        }
        section p {
            margin-bottom: 15px;
        }
    </style>
</head>
<body>
    <header>
        <h1>Exploring Depth and Width in Multi-Layer Perceptrons for Credit Risk Prediction</h1>
        <h3>Name: Cynthia Chinenye Udoye</h3>
        <h3>Student No: 22029346</h3>
    </header>

    <section id="abstract">
        <h2>Abstract</h2>
        <p>
            This tutorial explores how the architecture of Multi-Layer Perceptrons (MLPs), specifically their depth (number of layers) and width (number of neurons per layer), impacts their performance in predicting credit risk. Using the "Credit-G" dataset, we address class imbalance by applying class weights and test various combinations of layers and neurons. Our findings reveal that while increasing the depth of MLPs enhances their learning capacity, it also increases the risk of overfitting. Similarly, adding more neurons often increases computational time without consistently improving accuracy. The best-performing model achieved an AUC-ROC score of 0.80. This tutorial provides a clear and practical guide for using neural networks to tackle imbalanced data challenges in credit risk prediction.
        </p>
    </section>

    <section id="introduction">
        <h2>Introduction</h2>
        <p>
            Predicting credit risk is very important in finance. It helps banks and other financial institutions figure out which applicants are likely to repay loans, reducing the risk of losing money and increasing profits (Thomas et al., 2002). Credit risk models make the decision-making process in lending more reliable.
        </p>
        <p>
            In this tutorial, we explore the use of Multi-Layer Perceptrons (MLPs), a type of neural network, for classifying credit data into 'good' and 'bad' risk categories. MLPs are particularly well-suited for this task due to their ability to model complex nonlinear relationships in data (LeCun et al., 2015). However, datasets like the "Credit-G" dataset often exhibit class imbalance, with significantly more 'good' cases than 'bad' (He and Garcia, 2009). To address this, we apply class weights to ensure equitable learning from both classes.
        </p>
        <p>
            The objective of this tutorial is to investigate how the architecture of MLPs, specifically the number of layers (depth) and neurons per layer (width), influences model performance. While deeper networks can capture intricate patterns and wider layers enhance representational power, both architectures pose challenges, including increased computational cost and potential overfitting (Hinton et al., 2012). Performance will be assessed using metrics such as AUC-ROC, accuracy, precision, recall, F1-score, and validation loss, providing a comprehensive evaluation of these trade-offs.
        </p>
        <p>
            This tutorial will take you through every step of the process, including understanding the dataset, preparing the data, testing different MLP designs, and looking at the results. By the end, we hope to gain valuable insights into building effective neural networks for credit risk prediction and similar challenges in finance.
        </p>
    </section>
    <section id="dataset-preprocessing">
    <h2>Dataset and Preprocessing</h2>
    <p>
        The "Credit-G" dataset from OpenML contains financial records labelled as 'good' or 'bad' credit risks. It is commonly used in credit risk modelling because it reflects real-world lending data. However, the dataset is imbalanced, with more 'good' cases than 'bad,' making it harder for models to learn from the minority class.
    </p>
    <p>
        The dataset includes both categorical and numerical features. The following preprocessing steps were applied:
    </p>
    <ol>
        <li><strong>One-Hot Encoding:</strong> Categorical features were converted into numerical values to make them usable by the model.</li>
        <li><strong>Scaling:</strong> Numerical features were standardised using <code>StandardScaler</code> to ensure that no single feature dominated the learning process.</li>
        <li><strong>Data Splitting:</strong> The dataset was divided into training, validation, and test sets using stratified sampling to maintain the class distribution across all subsets.</li>
        <li><strong>Class Weights:</strong> Class weights were calculated and applied during training to give equal importance to 'good' and 'bad' cases.</li>
    </ol>
    <p>
        <strong>Key Considerations:</strong><br>
        Reproducibility was ensured by using consistent random seeds for data splitting and preprocessing. Ethical considerations, such as avoiding bias amplification, were kept in mind to ensure fair treatment of both classes in the dataset.
    </p>
</section>

<section id="model-architecture-design">
    <h2>Model Architecture and Design</h2>
    <p>
        The Multi-Layer Perceptrons (MLP) is a neural network that learns patterns in data. In this tutorial, the MLP was used to classify credit risks as 'good' or 'bad' based on the features of the "Credit-G" dataset (OpenML, n.d.).
    </p>

    <h3>1. Model Overview</h3>
    <ul>
        <li>
            <strong>Input Layer:</strong><br>
            Receives the processed dataset, with the number of input nodes equal to the total number of features.
        </li>
        <li>
            <strong>Hidden Layers:</strong><br>
            <ul>
                <li>Multiple hidden layers were tested, varying in the number of layers (depth) and neurons per layer (width).</li>
                <li>Each layer uses the ReLU (Rectified Linear Unit) activation function to identify complex patterns (LeCun et al., 2015).</li>
            </ul>
        </li>
        <li>
            <strong>Output Layer:</strong><br>
            Contains one neuron with a sigmoid activation function, predicting the probability of a record being a 'bad' credit risk.
        </li>
    </ul>

    <p>
        <strong>Figure 1:</strong> The structure of Multi-Layer Perceptrons (MLP) consists of interconnected layers facilitating complex computations (Datacamp, n.d.).
    </p>

    <h3>2. Improving the Model</h3>
    <ul>
        <li>
            <strong>Dropout:</strong><br>
            Randomly disables some neurons during training to prevent the model from relying too heavily on specific patterns (Srivastava et al., 2014).
        </li>
        <li>
            <strong>L2 Regularisation:</strong><br>
            Adds a penalty for large weights, helping reduce overfitting and improve the model's ability to generalise to new data (Hinton et al., 2012).
        </li>
    </ul>

    <h3>3. Impact of Depth and Width</h3>
    <ul>
        <li>
            <strong>Depth (Layers):</strong><br>
            Adding more layers helps the model learn more detailed patterns but can lead to overfitting and slower training if overdone.
        </li>
        <li>
            <strong>Width (Neurons per Layer):</strong><br>
            Increasing neurons gives the model more capacity to learn but also increases training time without always improving performance.
        </li>
        <li>
            <strong>Optimal Design:</strong><br>
            Tests showed that a model with 4 layers and 64 neurons per layer achieved the best trade-off between performance and complexity (Goodfellow et al., 2016).
        </li>
    </ul>

    <h3>4. Using Class Weights</h3>
    <p>
        The "Credit-G" dataset is imbalanced, with more 'good' cases than 'bad.' Class weights were applied to make the model focus more on the minority class ('bad'). This improved the recall for 'bad' cases, balancing the modelâ€™s predictions (He & Garcia, 2009).
    </p>
</section>

</body>
</html>
